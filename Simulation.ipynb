{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Dense, Input, Lambda, Flatten, Conv2D\n",
    "from tensorflow.keras import Model\n",
    "from tensorflow.keras.layers import concatenate\n",
    "from tensorflow.keras.utils import plot_model\n",
    "\n",
    "import ot  \n",
    "import ot.plot\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mitdeeplearning import util\n",
    "import os\n",
    "import time\n",
    "import functools\n",
    "from IPython import display as ipythondisplay\n",
    "from IPython.display import clear_output\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def discrete_gaussian(mean, variance, interval): \n",
    "    f = np.exp(-(interval-mean)**2/(2*variance)) / np.sqrt(2*np.pi*variance)\n",
    "    return f/f.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# experimental distriputions\n",
    "interval = np.linspace(0, 10, 50, dtype='float32')\n",
    "\n",
    "A = discrete_gaussian(7, 0.5, interval)\n",
    "B = discrete_gaussian(3, 0.5, interval)\n",
    "C = discrete_gaussian(5, 0.5, interval)\n",
    "D = discrete_gaussian(8, 0.5, interval)\n",
    "MU = [discrete_gaussian(i, 0.5, interval) for i in np.linspace(3, 8, 21)]\n",
    "\n",
    "plt.figure()\n",
    "for m in MU:\n",
    "    plt.plot(interval, m)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test cost functions\n",
    "c_f1 = lambda x,y: (x-y)**2  # distance squared\n",
    "c_f2 = lambda x, y: 1/(1+np.abs(x-y)) # coulomb cost\n",
    "\n",
    "def construct_tensor(x, n_marginals, func): #creates cost tensor from a base function\n",
    "    X = []\n",
    "    C = 0\n",
    "    for i in range(n_marginals):\n",
    "        shape = [1]*n_marginals\n",
    "        shape[i] = len(x)\n",
    "        X.append(np.reshape(x, tuple(shape)))\n",
    "        \n",
    "    for i in range(n_marginals-1):\n",
    "        for j in range(i+1, n_marginals):\n",
    "            C = C+func(X[i], X[j])\n",
    "            \n",
    "    return C\n",
    "\n",
    "# test Cost tensors for different number of marginals\n",
    "C1 = [construct_tensor(interval, n, c_f1) for n in range(2, 6)]\n",
    "C2 = [construct_tensor(interval, n, c_f2) for n in range(2, 6)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Neural nerwork architecture\n",
    "class OT_network:\n",
    "    def __init__(self, C, eps):\n",
    "        self.C = C\n",
    "        self.eps = eps\n",
    "        self.K = np.exp(-C/C.max()/eps)\n",
    "        self.n_marginals = len(C.shape)\n",
    "        self.shapes = (self.n_marginals, C.shape[0], )\n",
    "        \n",
    "        self.model = self.build_model()\n",
    "        self.optimizer = tf.keras.optimizers.Adam(learning_rate = 0.01)\n",
    "    \n",
    "    def build_model(self): #model via functional api \n",
    "        input_msrs = Input(shape=self.shapes, name=\"Marginals\") # takes array of marginal measures\n",
    "        flat_msrs = Flatten(name=\"Marginals_flat\")(input_msrs) # flattens the marginals into 1 array\n",
    "        log_msrs = Lambda(lambda x: tf.math.log(tf.cast(x, tf.float32)), name=\"logM\")(flat_msrs) # additional input log\n",
    "        new_inputs = concatenate([flat_msrs, log_msrs], name=\"M_concat_logM\") # makes a new kernel that has MU, log(MU)\n",
    "        \n",
    "        hidden1 = Dense(20, activation='relu', name=\"hidden1\", kernel_initializer='zeros')(new_inputs) \n",
    "        \n",
    "        output_potentials = [Dense(self.shapes[1], name=f\"u{i}\", kernel_initializer='zeros', \n",
    "                         bias_initializer='zeros')(hidden1) for i in range(1, self.n_marginals)] # outputs N-1 predicted U\n",
    "        \n",
    "        model = Model(inputs=[input_msrs], outputs=output_potentials) #stacks all layers together\n",
    "        return model\n",
    "    \n",
    "    @staticmethod\n",
    "    def plot_P(mu, nu, P): # plots estimated Coupling projected on first 2 axes\n",
    "        plt.figure(figsize=(7,7))\n",
    "        ot.plot.plot1D_mat(mu, nu, P.sum(axis=tuple([i for i in range(2, len(P.shape))])))\n",
    "        return plt\n",
    "        \n",
    "    def train_step(self, marginals): #optimization step\n",
    "        potentials = self.predict_potentials(marginals)\n",
    "        P = self.predict_P(potentials)\n",
    "        with tf.GradientTape() as tape: #assign loss function\n",
    "            loss = self.loss(marginals)\n",
    "            \n",
    "        grads = tape.gradient(loss, self.model.trainable_variables)\n",
    "        self.optimizer.apply_gradients(zip(grads, self.model.trainable_variables))\n",
    "        return loss, potentials, P\n",
    "        \n",
    "    def fit(self, marginals, epochs=100): # model train\n",
    "        plotter = util.PeriodicPlotter(sec=1, xlabel='Iterations', ylabel='Loss')\n",
    "        if hasattr(tqdm, '_instances'): tqdm._instances.clear() # clear if it exists\n",
    "\n",
    "        for iter in tqdm(range(epochs)):\n",
    "            loss, potentials, P = self.train_step(marginals)\n",
    "            clear_output(wait=True)\n",
    "            fig = self.plot_P(marginals[0, 0], marginals[0,1], P[0].numpy())\n",
    "#             fig.savefig(f\"C2\\\\dnn\\\\N={self.n_marginals}\\\\{iter}.png\")\n",
    "            fig.show()\n",
    "        \n",
    "        return potentials, P.numpy()\n",
    "    \n",
    "    def U_call(self, marginals): #estimate u_1, ..., u_n-1 \n",
    "        prediction = self.model.call({\"Marginals\": tf.Variable(marginals)})   \n",
    "        return prediction if type(prediction)== list else [prediction] \n",
    "    \n",
    "    def call(self, marginals): # predict P(marginals)\n",
    "        potentials = self.predict_potentials(marginals)\n",
    "        return self.predict_P(potentials).numpy()\n",
    "    \n",
    "    def to_tensors(self, vec, pos): # reshapes vectors to do elementwise operations with tensors for all samples\n",
    "        shape = [1]*(1+self.n_marginals)\n",
    "        n_samples, n = vec.numpy().shape\n",
    "        \n",
    "        shape[0] = n_samples\n",
    "        shape[pos+1] = n\n",
    "        return tf.reshape(vec, tuple(shape))\n",
    "    \n",
    "    @property\n",
    "    def K_reshaped(self): # reshapes exp(-Cost) tensor to do operations for all samples\n",
    "        return tf.reshape(self.K, (1,)+self.K.shape)\n",
    "    \n",
    "    def dual(self, marginals): # entropic dual functional for sample\n",
    "        U = self.predict_potentials(marginals)\n",
    "        P = self.predict_P(U)\n",
    "        D = - self.eps*tf.reduce_sum(P, axis=range(1,self.n_marginals+1))\n",
    "        for i, u in enumerate(U):\n",
    "            D += tf.reduce_sum(u*marginals[:, i], axis=1)\n",
    "        return D \n",
    "\n",
    "    def loss(self, marginals):\n",
    "        return -tf.reduce_sum(self.dual(marginals))\n",
    "    \n",
    "    def predict_potentials(self, marginals): # estimate all potentials \n",
    "        U = self.U_call(marginals)\n",
    "        factor = self.K_reshaped\n",
    "        potentials = []\n",
    "        \n",
    "        # estimate u_n from u_1... u_n-1\n",
    "        for i, u in enumerate(U):\n",
    "            potentials.append(u)\n",
    "            factor = factor * self.to_tensors(tf.math.exp(u/self.eps), i)\n",
    "            \n",
    "        u_n = self.eps*(tf.math.log(marginals[:, -1]) - tf.math.log(tf.reduce_sum(factor, axis=range(1, self.n_marginals))))\n",
    "        potentials.append(u_n)\n",
    "        return potentials \n",
    "    \n",
    "    def predict_P(self, potentials): # estimate P(potentials)\n",
    "        factor = self.K_reshaped\n",
    "        for i, u in enumerate(potentials):\n",
    "            factor = factor * self.to_tensors(tf.math.exp(u/self.eps), i)\n",
    "        return factor\n",
    "    \n",
    "    @property\n",
    "    def plot_model(self): #plots the netrork structure\n",
    "        return plot_model(self.model, show_shapes=True)\n",
    "    \n",
    "    @staticmethod\n",
    "    def plot_cost(C): # plots Cost tensor projected on first 2 axes\n",
    "        plt.figure(figsize=(7,7))\n",
    "        plt.imshow(C.sum(axis=tuple([i for i in range(2, len(C.shape))])))\n",
    "        plt.title(\"Cost function\")\n",
    "        return plt.show()\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dual_sinkhorn(marginals, C, eps, rate, potentials=None):\n",
    "    K = np.exp(-C/C.max()/eps)\n",
    "    N = len(C.shape)\n",
    "    length = C.shape[0]\n",
    "    \n",
    "    if potentials == None:\n",
    "        potentials = np.zeros((N, length))\n",
    "    else:\n",
    "        potentials = np.reshape(potentials, (N, length))\n",
    "        \n",
    "    P = K\n",
    "    for i, u in enumerate(potentials):\n",
    "        shape = [1]*N\n",
    "        shape[i] = length\n",
    "        P = P * np.reshape(np.exp(u/eps), tuple(shape))\n",
    "    \n",
    "    it = 0\n",
    "    fig = net.plot_P(marginals[0], marginals[1], P)\n",
    "#     fig.savefig(f\"C2\\\\sinkhorn\\\\N={N}\\\\{it}.png\")\n",
    "    err = np.max([np.abs(P.sum(axis=tuple(np.delete(range(N), i))) - mu) for i, mu in enumerate(marginals)])\n",
    "    while err>= rate:\n",
    "        for i in range(N):\n",
    "            shape = [1]*N\n",
    "            shape[i] = length\n",
    "            P = P / np.reshape(np.exp(potentials[i]/eps), tuple(shape))\n",
    "            u = eps*np.log(marginals[i]) -eps*np.log(P.sum(axis=tuple(np.delete(range(N), i))))\n",
    "            potentials[i] = u\n",
    "            P = P * np.reshape(np.exp(u/eps), tuple(shape))\n",
    "        it +=1\n",
    "        err = np.max([np.abs(P.sum(axis=tuple(np.delete(range(N), i))) - mu) for i, mu in enumerate(marginals)])\n",
    "        fig = net.plot_P(marginals[0], marginals[1], P)\n",
    "#         fig.savefig(f\"C2\\\\sinkhorn\\\\N={N}\\\\{it}.png\")\n",
    "    print(\"iters:\", it)       \n",
    "    return potentials, P"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Coulomb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "OT_network.plot_cost(C2[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = OT_network(C2[1], 0.01)\n",
    "net.plot_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "potentials, P = net.fit(np.array([np.stack([MU[0], MU[0], MU[0]]), np.stack([MU[10], MU[10], MU[10]])]), epochs=80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# simulaiton for dissociation (change of marginal)\n",
    "for i in range(1,21):\n",
    "    _potentials, _P = net.fit(np.array([np.stack([MU[0], MU[i], MU[0]])]), epochs=10)\n",
    "#     fig = net.plot_P(MU[i], MU[20], _P[0])\n",
    "#     fig.savefig(f\"C2\\\\dnn\\\\Dispersion\\\\3AB-{20+i}.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sinkhorn for comparison\n",
    "potentials_sink, P_sink = dual_sinkhorn([A, A, A], C2[1], 0.01, 0.001)\n",
    "net.plot_P(A, A, P_sink)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
