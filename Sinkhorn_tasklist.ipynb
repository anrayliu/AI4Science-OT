{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (!) Remark this file is to be updated from time to time\n",
    "\n",
    "# $2$ marginals\n",
    "\n",
    "Recall the Primal-Dual entropy regularized problems.\n",
    "\n",
    "### The regularized Primal problem \n",
    "\n",
    "Fix $\\varepsilon>0$. We are looking for a solution of\n",
    "\n",
    "$P^{\\varepsilon}\\in \\argmax \\{F^{\\varepsilon}_C(P) := \\sum\\limits_{i=0}^{n-1}\\sum\\limits_{j=0}^{m-1}C_{ij}P_{ij} +\\varepsilon \\sum\\limits_{i=0}^{n-1}\\sum\\limits_{j=0}^{m-1} P_{ij}(\\log P_{ij}-1)\\,:\\, P_{ij}\\geq 0, \\, \\sum\\limits_{i=0}^{n-1} P_{ij} = \\nu_{j}, \\, \\sum\\limits_{j=0}^{m-1} P_{ij} = \\mu_i\\}$\n",
    "\n",
    "\n",
    "### The regularized Dual problem\n",
    "\n",
    "Consider a function $D^{\\varepsilon}_{C}:\\mathbb{R}^n \\times \\mathbb{R}^m \\to \\mathbb{R}:$\n",
    "\n",
    "$D^{\\varepsilon}_{C}(u,v) = \\sum_{i=0}^{n-1} u_i \\mu_i + \\sum_{j=n}^{m-1} v_j \\nu_j - \\varepsilon \\sum_{i=0}^{n-1}\\sum_{j=0}^{m-1} e^{\\frac{u_i+v_j - C_{ij}}{\\varepsilon}}$\n",
    "\n",
    "#### Facts (Optimization Theory): \n",
    "1. It turns out that\n",
    "\n",
    "$$\\min \\{F^{\\varepsilon}_C(P) \\,:\\, P_{ij}\\geq 0, \\, \\sum\\limits_{i=0}^{n-1} P_{ij} = \\nu_{j}, \\, \\sum\\limits_{j=0}^{m-1} P_{ij} = \\mu_i\\} = \\max \\{ D^{\\varepsilon}_C(u,v) \\,:\\, u\\in \\mathbb{R}^n,~ v\\in \\mathbb{R}^m\\}.$$\n",
    "\n",
    "2. There exists a unique minimizer $P^{\\varepsilon}$ of $F^{\\varepsilon}_C$ and it has a shape \n",
    "$$ \n",
    "P^{\\varepsilon}_{ij}  = e^{\\frac{u^{\\varepsilon}_i+v^{\\varepsilon}_j - C_{ij}}{\\varepsilon}},\n",
    "$$\n",
    "where $u^{\\varepsilon}_i$ and $v^{\\varepsilon}_j$ are (any) maximizers of the Dual problem $D^{\\varepsilon}_C$. \n",
    "\n",
    "\n",
    "## Sinkhorn algorithm (2 marginals)\n",
    "*Idea:* Do the alternate maximization of the functional $D^\\varepsilon_C$.\n",
    "\n",
    "`Start`: Define $(u^{(0)}, v^{(0)}) = (0, 0)$.\n",
    "\n",
    "`Iterations`: for $l=1, 2, ...$ do\n",
    "1. Compute $u^{(l)} = \\argmax \\{ D^{\\varepsilon}_C (u, v^{(l-1)}) \\,:\\, u \\in \\mathbb{R^n}\\}$.\n",
    "2. Compute $v^{(l)} = \\argmax \\{ D^{\\varepsilon}_C (u^{(l)}, v) \\,:\\, v \\in \\mathbb{R}^m\\}$.\n",
    "3. (Optional) Compute $P^{(l)} = e^{\\frac{u^{(l)}\\oplus v^{(l)} - C}{\\varepsilon}}$\n",
    "\n",
    "There is an explicit formula for those $\\argmax D^\\varepsilon_C$ -- see the presentation.\n",
    "\n",
    "Typically, in practice we do some finite number of iterations, because in fact after some point $P^{(l)}$ doesn't differ very much from $P^{(l-1)}$.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# $N$ marginals\n",
    "\n",
    "Now we arrive to a point when we can extend our problem to have more than 2 \"marginal\" distributions. \n",
    "\n",
    "Let us have\n",
    "$\\mu^1 = (\\mu^1_0, \\dots, \\mu^1_{n-1}), ~\\mu^2 =  (\\mu^2_0, \\dots, \\mu^2_{n-1}), \\dots,  \\mu^N = (\\mu^N_0, \\dots, \\mu^N_{n-1})\\quad$          (for simplicity let's say they have the same dimension $n$)\n",
    "\n",
    "### The regularized Primal problem \n",
    "\n",
    "Fix $\\varepsilon>0$. We are looking for a solution of\n",
    "\n",
    "$P^{\\varepsilon}\\in \\argmax \\{F^{\\varepsilon}_C(P) := \\sum\\limits_{i_1, \\dots, i_N=0}^{n-1}C_{i_1 \\dots i_N}P_{i_1\\dots i_N} +\\varepsilon \\sum\\limits_{i_1, \\dots, i_N=0}^{n-1} P_{i_1\\dots i_N}(\\log P_{i_1\\dots i_N}-1)\\,:\\, P_{i_1 \\dots i_N}\\geq 0, \\, \\sum\\limits_{i_1, \\dots i_{k-1} i_{k+1} \\dots i_N=0}^{n-1} P_{i_1\\dots i_{k-1} j i_{k+1} \\dots i_N} = \\mu^k_{j}, \\}$\n",
    "\n",
    "So here both $C$ and $P$ are the generalized matrices (tensors), which have $N$ indeces instead of $2$. Computationally, it is just an `np.ndarray`, e.g. if we have $N=5$ and $n=50$ then $P$ will have the shape (50,50,50,50,50).\n",
    "\n",
    "\n",
    "### The regularized Dual problem\n",
    "\n",
    "In a similar way, consider a function $D^{\\varepsilon}_{C}:\\mathbb{R}^n \\times \\dots\\times \\mathbb{R}^n  \\to \\mathbb{R}$ such that\n",
    "\n",
    "$D^{\\varepsilon}_{C}(u^1,\\dots, u^N) = \\sum_{i_1=0}^{n-1} u^1_{i_1} \\mu^1_i +\\dots+ \\sum_{i_N=0}^{n-1} u^N_i \\mu^N_{i_N} - \\varepsilon \\sum_{i_1,\\dots,i_N=1}^{n-1} e^{\\frac{u^1_{i_i}+\\dots+u^N_{i_N} - C_{i_1\\dots i_N}}{\\varepsilon}}$\n",
    "\n",
    "In the same way, we have the relation e.g. $\\min F^\\varepsilon_C(P) = \\max D^\\varepsilon_C(u^1,\\dots, u^N)$, the relations between the solutions of $\\min$ and $\\max$ problems\n",
    "$$P^{\\varepsilon}_{i_1\\dots i_N}  = e^{\\frac{u^{1, \\varepsilon}_{i_1}+\\dots + u^{N, \\varepsilon}_{i_N} - C_{i_1\\dots i_N}}{\\varepsilon}},$$\n",
    "and the idea of the algorithm\n",
    "\n",
    "## Sinkhorn algorithm (N marginals)\n",
    "*Idea:* Similarly, perform the alternate maximization of the functional $D^\\varepsilon_C$.\n",
    "\n",
    "`Start`: Define $(u^{1(0)}, \\dots, u^{N(0)}) = (0, \\dots, 0)$.\n",
    "\n",
    "`Iterations`: for $l=1, 2, ...$ do\n",
    "1. Compute $u^{1(l)} = \\argmax \\{ D^{\\varepsilon}_C (u, u^{2(l-1)}, u^{3(l-1)}, \\dots, u^{N(l-1)}) \\,:\\, u \\in \\mathbb{R^n}\\}$.\n",
    "2. Compute $u^{2(l)} = \\argmax \\{ D^{\\varepsilon}_C (u^{1(l)}, u, u^{3(l-1)}, \\dots, u^{N(l-1)}) \\,:\\, u \\in \\mathbb{R}^n\\}$.\n",
    "\n",
    "...\n",
    "\n",
    "3. Compute $u^{N(l)} = \\argmax \\{ D^{\\varepsilon}_C (u^{1(l)},\\dots, u^{(N-1)(l-1)}, u) \\,:\\, u \\in \\mathbb{R}^n\\}$.\n",
    "\n",
    "3. (Optional) Compute $P^{(l)} = e^{\\frac{u^{1(l)}\\oplus\\dots\\oplus u^{N(l)} - C}{\\varepsilon}}$\n",
    "\n",
    "### Good news: I have this algorithm already implemented! (See `simulation.ipynb`)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Summer project\n",
    "\n",
    "#### There is a certain tasklist that I would like you to understand/implement\n",
    "\n",
    "#### `1.` Try to understand the idea how the Sinkhorn algorithm works, i.e. play with the code, run iteration by iteration. You may make a copy of my function and edit it so it may take different parameters as inputs.\n",
    "\n",
    "#### `2.` 'Evolving marginals': let's take an example for $N=2$\n",
    "Idea: Assume that you calculated the \"solution\" of the problem for some $\\mu^1, \\mu^2$. Now, you are interested how the solution *will change* if you change a little bit $\\mu^2$. For this you may run the Sinkhorn algorithm for modified marginals and look at the new solution and compare them side by side. We can actually modify the way that I defined the Sinkhorn algorithm, e.g. to initialize with the \"previous\" solutions instead of zeros.\n",
    "\n",
    "What we can do more? Change $\\mu^2$ again a little bit and continue, on and on, and get a sequence of new solutions. Now it is very natural to see the animation of what was the *evolution* of the solutions, for example using `interact` or by making \"gifs\" of the new solutions ate the snapshots of the gif.\n",
    "\n",
    "What we can do even more? Yes, $N-$marginal :)\n",
    "\n",
    "#### `3.` Electron dissociations -- very nice example of the generalization of `2.` for N marginals\n",
    "Say we have $\\mu^1=\\mu^2=\\dots=\\mu^N=\\rho$ and we solve the problem. Now we want to start modifying $\\mu^N$ little by little and look at teh new solutions like we did in `2.` Next, we can also fix $\\mu^N = \\tilde{\\rho}$ at its last configuration, and instead start modifying $\\mu^{N-1}$ and go on. For example, we can modify it such that at some point also becomes $\\tilde{\\rho}$. \n",
    "\n",
    "I did those simulations using Neural Network as a solver, instead of Sinkhorn algorithm, in my slides, and you can also see it in the file `simulation.ipynb`. This time, I would like to do it using Sinkhorn, because it may *converge to the solution* faster than teh solution using a neural network.\n",
    "\n",
    "#### `4.` (Additional) Better way of plotting the \"evolutions\" of solutions --- Real coding assignment\n",
    "Right now there is a code that makes nice real-time updating plots of those \"updates\" of the solutions in the `simulation.ipynb`. However, it uses some tools from `mitdeeplearning` library. My wish is to avoid this library at all, and right now I think am using it only for those plots. (We may have a separate discussion on this later, we may need to look at some parts of my neural network class where my plotter is implemented)\n",
    "\n",
    "#### `5.` (If we have time): write the documentations for our functions so it is much simpler to understand each component.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
